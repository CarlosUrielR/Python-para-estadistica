{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning - aprendizaje supervisado\n",
    "\n",
    "Usamos datos del Lending Club para desarrollar nuestra comprensión de los conceptos de aprendizaje automático. The Lending Club es una compañía de préstamos peer-to-peer. Ofrece préstamos que son financiados por otras personas. En este sentido, el Lending Club actúa como un centro que conecta a los prestatarios con los inversores. El cliente solicita un préstamo de cierta cantidad y la empresa evalúa el riesgo de la operación. Si la solicitud es aceptada, puede o no estar completamente cubierta. Nos centraremos en la predicción de si el préstamo será financiado en su totalidad, en función de la calificación y la información relacionada con la aplicación. Los datos provienen de la siguiente URL:\n",
    "\n",
    "https://www.lendingclub.com/info/download-data.action\n",
    "\n",
    "Limpiaremos y reprocesaremos el conjunto de datos parcial de 2007-2011. Al enunciar el problema un poco más, de acuerdo con la información suministrada por el cliente que solicita un préstamo, queremos predecir si un préstamo aceptado se financiará por completo o no.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"> <sub>Referencia: Igual L. & Seguí, S. (2017). *Introduction to Data Science*. Springer. Código fuente: https://github.com/DataScienceUB/introduction-datascience-python-book </sub> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-19T13:56:56.166375Z",
     "start_time": "2018-09-19T13:56:56.152283Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "\n",
    "%matplotlib inline \n",
    "plt.style.use('seaborn-whitegrid')\n",
    "plt.rc('text', usetex=True)\n",
    "plt.rc('font', family='times')\n",
    "plt.rc('xtick', labelsize=10) \n",
    "plt.rc('ytick', labelsize=10) \n",
    "plt.rc('font', size=12) \n",
    "plt.rc('figure', figsize = (12, 5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-19T14:01:14.367699Z",
     "start_time": "2018-09-19T14:01:14.353715Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "ofname = open('dataset_small.pkl','rb') \n",
    "(x,y) = pickle.load(ofname, encoding='latin1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y revisemos las dimensiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-19T13:59:36.092843Z",
     "start_time": "2018-09-19T13:59:36.086823Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dims: 15, samples: 4140\n"
     ]
    }
   ],
   "source": [
    "dims = x.shape[1]\n",
    "N = x.shape[0]\n",
    "print('dims: ' + str(dims) + ', samples: ' + str(N))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Ajustemos un primer clasificador:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-19T14:04:03.330772Z",
     "start_time": "2018-09-19T14:04:03.257780Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted value: -1.0 , real target: -1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn import neighbors\n",
    "from sklearn import datasets\n",
    "# Create an instance of K-nearest neighbor classifier\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors=11)\n",
    "# Train the classifier\n",
    "knn.fit(x,y)\n",
    "# Compute the prediction according to the model\n",
    "yhat = knn.predict(x)\n",
    "print('Predicted value: ' + str(yhat[-1]), ', real target: ' + str(y[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y revisar su exactitud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-19T14:04:37.077923Z",
     "start_time": "2018-09-19T14:04:36.997231Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8316425120772947"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.score(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La distribución de las etiquetas para predecir se muestra en el siguiente gráfico de torta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-19T14:04:57.920134Z",
     "start_time": "2018-09-19T14:04:54.866093Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "#import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.pie(np.c_[np.sum(np.where(y==1,1,0)),np.sum(np.where(y==-1,1,0))][0],\n",
    "        labels=['Not fully funded','Full amount'],\n",
    "        colors=['g','r'],\n",
    "        shadow=False,\n",
    "        autopct ='%.2f' )\n",
    "plt.gcf().set_size_inches((6,6))\n",
    "plt.savefig(\"pie.png\",dpi=300, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y calculemos los valores de los elementos de la matriz de confusión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-19T14:05:43.945467Z",
     "start_time": "2018-09-19T14:05:43.848468Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "yhat = knn.predict(x)\n",
    "TP = np.sum(np.logical_and(yhat==-1,y==-1))\n",
    "TN = np.sum(np.logical_and(yhat==1,y==1))\n",
    "FP = np.sum(np.logical_and(yhat==-1,y==1))\n",
    "FN = np.sum(np.logical_and(yhat==1,y==-1))\n",
    "print( 'TP: '+ str(TP), ', FP: '+ str(FP))\n",
    "print( 'FN: '+ str(FN), ', TN: '+ str(TN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-19T14:05:49.505218Z",
     "start_time": "2018-09-19T14:05:49.492691Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "metrics.confusion_matrix(yhat,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-19T14:06:40.204292Z",
     "start_time": "2018-09-19T14:06:40.153395Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Train a classifier using .fit()\n",
    "from sklearn import neighbors\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(x,y)\n",
    "yhat=knn.predict(x)\n",
    "\n",
    "print(\"classification accuracy:\", metrics.accuracy_score(yhat, y))\n",
    "print(\"confusion matrix: \\n\" + str(metrics.confusion_matrix(yhat, y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasta este momento, utilizamos datos de entrenamiento para \"evaluar\" el rendimiento del método, como veremos más adelante, esta es una mala práctica. Simulemos la etapa de explotación al mantener un subconjunto de los datos de entrenamiento y evaluar el rendimiento en ese conjunto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-19T14:07:40.984397Z",
     "start_time": "2018-09-19T14:07:40.923109Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Simulate a real case: Randomize and split data in two subsets PRC*100% for training and \n",
    "# the rest (1-PRC)*100% for testing\n",
    "import numpy as np\n",
    "perm = np.random.permutation(y.size)\n",
    "PRC = 0.7\n",
    "split_point = int(np.ceil(y.shape[0]*PRC))\n",
    "\n",
    "X_train = x[perm[:split_point].ravel(),:]\n",
    "y_train = y[perm[:split_point].ravel()]\n",
    "\n",
    "X_test = x[perm[split_point:].ravel(),:]\n",
    "y_test = y[perm[split_point:].ravel()]\n",
    "\n",
    "print('Training shape: ' + str(X_train.shape), ' , training targets shape: '+str(y_train.shape))\n",
    "print('Testing shape: ' + str(X_test.shape), ' , testing targets shape: '+str(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-19T14:08:24.505407Z",
     "start_time": "2018-09-19T14:08:24.464454Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Train a classifier on training data\n",
    "from sklearn import neighbors\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(X_train,y_train)\n",
    "\n",
    "#Check on the training set and visualize performance\n",
    "yhat=knn.predict(X_train)\n",
    "\n",
    "from sklearn import metrics\n",
    "print(\"\\nTRAINING STATS:\")\n",
    "print(\"classification accuracy:\", metrics.accuracy_score(yhat, y_train))\n",
    "print(\"confusion matrix: \\n\"+ str(metrics.confusion_matrix(y_train, yhat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-19T14:09:05.986596Z",
     "start_time": "2018-09-19T14:09:05.959931Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Check on the test set\n",
    "yhat=knn.predict(X_test)\n",
    "print(\"TESTING STATS:\")\n",
    "print(\"classification accuracy:\", metrics.accuracy_score(yhat, y_test))\n",
    "print(\"confusion matrix: \\n\"+ str(metrics.confusion_matrix(yhat,y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos automatizar este proceso con las herramientas proporcionadas en sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-19T14:10:34.914422Z",
     "start_time": "2018-09-19T14:10:34.734701Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#The splitting can be done using the tools provided by sklearn:\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import neighbors\n",
    "from sklearn import metrics\n",
    "\n",
    "PRC = 0.3\n",
    "acc = np.zeros((10,))\n",
    "for i in range(10):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=PRC)\n",
    "    knn = neighbors.KNeighborsClassifier(n_neighbors=1)\n",
    "    knn.fit(X_train,y_train)\n",
    "    yhat = knn.predict(X_test)\n",
    "    acc[i] = metrics.accuracy_score(yhat, y_test)\n",
    "acc.shape=(1,10)\n",
    "print(\"Mean expected error: \"+str(np.mean(acc[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos usar el proceso de validación para la selección del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-19T14:11:56.586678Z",
     "start_time": "2018-09-19T14:11:46.204191Z"
    }
   },
   "outputs": [],
   "source": [
    "#The splitting can be done using the tools provided by sklearn:\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import neighbors\n",
    "from sklearn import tree\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "\n",
    "PRC = 0.1\n",
    "acc_r=np.zeros((10,4))\n",
    "for i in range(10):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=PRC)\n",
    "    nn1 = neighbors.KNeighborsClassifier(n_neighbors=1)\n",
    "    nn3 = neighbors.KNeighborsClassifier(n_neighbors=3)\n",
    "    svc = svm.SVC()\n",
    "    dt = tree.DecisionTreeClassifier()\n",
    "    \n",
    "    nn1.fit(X_train,y_train)\n",
    "    nn3.fit(X_train,y_train)\n",
    "    svc.fit(X_train,y_train)\n",
    "    dt.fit(X_train,y_train)\n",
    "    \n",
    "    yhat_nn1=nn1.predict(X_test)\n",
    "    yhat_nn3=nn3.predict(X_test)\n",
    "    yhat_svc=svc.predict(X_test)\n",
    "    yhat_dt=dt.predict(X_test)\n",
    "    \n",
    "    acc_r[i][0] = metrics.accuracy_score(yhat_nn1, y_test)\n",
    "    acc_r[i][1] = metrics.accuracy_score(yhat_nn3, y_test)\n",
    "    acc_r[i][2] = metrics.accuracy_score(yhat_svc, y_test)\n",
    "    acc_r[i][3] = metrics.accuracy_score(yhat_dt, y_test)\n",
    "\n",
    "\n",
    "plt.boxplot(acc_r);\n",
    "for i in range(4):\n",
    "    xderiv = (i+1)*np.ones(acc_r[:,i].shape)+(np.random.rand(10,)-0.5)*0.1\n",
    "    plt.plot(xderiv,acc_r[:,i],'ro',alpha=0.3)\n",
    "    \n",
    "ax = plt.gca()\n",
    "ax.set_xticklabels(['1-NN','3-NN','SVM','Decission Tree'])\n",
    "plt.ylabel('Accuracy')\n",
    "plt.savefig(\"error_ms_1.png\",dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curvas de aprendizaje\n",
    "\n",
    "Intentemos comprender el comportamiento de los algoritmos de aprendizaje automático cuando cambia la cantidad de datos y la \"complejidad\" del método. Comencemos primero variando la cantidad de datos para una complejidad fija."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-19T14:14:40.286430Z",
     "start_time": "2018-09-19T14:14:39.189872Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "MAXN=700\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(6,5)\n",
    "\n",
    "plt.plot(1.25*np.random.randn(MAXN,1),1.25*np.random.randn(MAXN,1),'r.',alpha = 0.3)\n",
    "#fig.hold('on')\n",
    "plt.plot(8+1.5*np.random.randn(MAXN,2),5+1.5*np.random.randn(MAXN,2),'r.', alpha = 0.3)\n",
    "plt.plot(5+1.5*np.random.randn(MAXN,1),5+1.5*np.random.randn(MAXN,1),'b.',alpha = 0.3)\n",
    "plt.savefig(\"toy_problem.png\",dpi=300, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-19T14:15:21.884030Z",
     "start_time": "2018-09-19T14:15:02.809465Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn import tree\n",
    "\n",
    "C=5\n",
    "MAXN=1000\n",
    "\n",
    "yhat_test=np.zeros((10,299,2))\n",
    "yhat_train=np.zeros((10,299,2))\n",
    "#Repeat ten times to get smooth curves\n",
    "for i in range(10):\n",
    "    X = np.concatenate([1.25*np.random.randn(MAXN,2),5+1.5*np.random.randn(MAXN,2)]) \n",
    "    X = np.concatenate([X,[8,5]+1.5*np.random.randn(MAXN,2)])\n",
    "    y = np.concatenate([np.ones((MAXN,1)),-np.ones((MAXN,1))])\n",
    "    y = np.concatenate([y,np.ones((MAXN,1))])\n",
    "    perm = np.random.permutation(y.size)\n",
    "    X = X[perm,:]\n",
    "    y = y[perm]\n",
    "\n",
    "    X_test = np.concatenate([1.25*np.random.randn(MAXN,2),5+1.5*np.random.randn(MAXN,2)]) \n",
    "    X_test = np.concatenate([X_test,[8,5]+1.5*np.random.randn(MAXN,2)])\n",
    "    y_test = np.concatenate([np.ones((MAXN,1)),-np.ones((MAXN,1))])\n",
    "    y_test = np.concatenate([y_test,np.ones((MAXN,1))])\n",
    "    j=0\n",
    "    for N in range(10,3000,10):\n",
    "        Xr=X[:N,:]\n",
    "        yr=y[:N]\n",
    "        #Evaluate the model\n",
    "        clf = tree.DecisionTreeClassifier(min_samples_leaf=1, max_depth=C)\n",
    "        clf.fit(Xr,yr.ravel())\n",
    "        yhat_test[i,j,0] = 1. - metrics.accuracy_score(clf.predict(X_test), y_test.ravel())\n",
    "        yhat_train[i,j,0] = 1. - metrics.accuracy_score(clf.predict(Xr), yr.ravel())\n",
    "        j=j+1\n",
    "\n",
    "p1,=plt.plot(np.mean(yhat_test[:,:,0].T,axis=1),'pink')\n",
    "p2,=plt.plot(np.mean(yhat_train[:,:,0].T,axis=1),'c')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(12,5)\n",
    "plt.xlabel('Number of samples x10')\n",
    "plt.ylabel('Error rate')\n",
    "plt.legend([p1,p2],[\"Test C = 5\",\"Train C = 5\"])\n",
    "plt.savefig(\"learning_curve_1.png\",dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repitamos el proceso con un modelo más sencillo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-19T14:15:51.405569Z",
     "start_time": "2018-09-19T14:15:42.151384Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "C=1\n",
    "MAXN=1000\n",
    "\n",
    "#Repeat ten times to get smooth curves\n",
    "for i in range(10):\n",
    "    X = np.concatenate([1.25*np.random.randn(MAXN,2),5+1.5*np.random.randn(MAXN,2)]) \n",
    "    X = np.concatenate([X,[8,5]+1.5*np.random.randn(MAXN,2)])\n",
    "    y = np.concatenate([np.ones((MAXN,1)),-np.ones((MAXN,1))])\n",
    "    y = np.concatenate([y,np.ones((MAXN,1))])\n",
    "    perm = np.random.permutation(y.size)\n",
    "    X = X[perm,:]\n",
    "    y = y[perm]\n",
    "\n",
    "    X_test = np.concatenate([1.25*np.random.randn(MAXN,2),5+1.5*np.random.randn(MAXN,2)]) \n",
    "    X_test = np.concatenate([X_test,[8,5]+1.5*np.random.randn(MAXN,2)])\n",
    "    y_test = np.concatenate([np.ones((MAXN,1)),-np.ones((MAXN,1))])\n",
    "    y_test = np.concatenate([y_test,np.ones((MAXN,1))])\n",
    "    j=0\n",
    "    for N in range(10,3000,10):\n",
    "        Xr=X[:N,:]\n",
    "        yr=y[:N]\n",
    "        #Evaluate the model\n",
    "        clf = tree.DecisionTreeClassifier(min_samples_leaf=1, max_depth=C)\n",
    "        clf.fit(Xr,yr.ravel())\n",
    "        yhat_test[i,j,1] = 1. - metrics.accuracy_score(clf.predict(X_test), y_test.ravel())\n",
    "        yhat_train[i,j,1] = 1. - metrics.accuracy_score(clf.predict(Xr), yr.ravel())\n",
    "        j=j+1\n",
    "\n",
    "p3,=plt.plot(np.mean(yhat_test[:,:,1].T,axis=1),'r')\n",
    "p4,=plt.plot(np.mean(yhat_train[:,:,1].T,axis=1),'b')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(12,5)\n",
    "plt.xlabel('Number of samples x10')\n",
    "plt.ylabel('Error rate')\n",
    "plt.legend([p3,p4],[\"Test C = 1\",\"Train C = 1\"])\n",
    "plt.savefig(\"learning_curve_2.png\",dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y juntemos los resultados para ver las diferencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-19T14:16:11.369517Z",
     "start_time": "2018-09-19T14:16:10.117381Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "p1,=plt.plot(np.mean(yhat_test[:,:,0].T,axis=1),color='pink')\n",
    "p2,=plt.plot(np.mean(yhat_train[:,:,0].T,axis=1),'c')\n",
    "p3,=plt.plot(np.mean(yhat_test[:,:,1].T,axis=1),'r')\n",
    "p4,=plt.plot(np.mean(yhat_train[:,:,1].T,axis=1),'b')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(12,5)\n",
    "plt.xlabel('Number of samples x10')\n",
    "plt.ylabel('Error rate')\n",
    "plt.legend([p1,p2,p3,p4],[\"Test C = 5\",\"Train C = 5\",\"Test C = 1\",\"Train C = 1\"])\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(12,5)\n",
    "plt.savefig(\"learning_curve_3.png\",dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos ahora qué sucede cuando corregimos la cantidad de datos y cambiamos la complejidad de la técnica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-19T14:18:41.388372Z",
     "start_time": "2018-09-19T14:18:39.530282Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%reset -f\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact\n",
    "from sklearn import metrics\n",
    "from sklearn import tree\n",
    "\n",
    "MAXC=20\n",
    "N=1000\n",
    "NTEST=4000\n",
    "ITERS=3\n",
    "\n",
    "yhat_test=np.zeros((ITERS,MAXC,2))\n",
    "yhat_train=np.zeros((ITERS,MAXC,2))\n",
    "#Repeat ten times to get smooth curves\n",
    "for i in range(ITERS):\n",
    "    X = np.concatenate([1.25*np.random.randn(N,2),5+1.5*np.random.randn(N,2)]) \n",
    "    X = np.concatenate([X,[8,5]+1.5*np.random.randn(N,2)])\n",
    "    y = np.concatenate([np.ones((N,1)),-np.ones((N,1))])\n",
    "    y = np.concatenate([y,np.ones((N,1))])\n",
    "    perm = np.random.permutation(y.size)\n",
    "    X = X[perm,:]\n",
    "    y = y[perm]\n",
    "\n",
    "    X_test = np.concatenate([1.25*np.random.randn(NTEST,2),5+1.5*np.random.randn(NTEST,2)]) \n",
    "    X_test = np.concatenate([X_test,[8,5]+1.5*np.random.randn(NTEST,2)])\n",
    "    y_test = np.concatenate([np.ones((NTEST,1)),-np.ones((NTEST,1))])\n",
    "    y_test = np.concatenate([y_test,np.ones((NTEST,1))])\n",
    "\n",
    "    j=0\n",
    "    for C in range(1,MAXC+1):\n",
    "        #Evaluate the model\n",
    "        clf = tree.DecisionTreeClassifier(min_samples_leaf=1, max_depth=C)\n",
    "        clf.fit(X,y.ravel())\n",
    "        yhat_test[i,j,0] = 1. - metrics.accuracy_score(clf.predict(X_test), y_test.ravel())\n",
    "        yhat_train[i,j,0] = 1. - metrics.accuracy_score(clf.predict(X), y.ravel())\n",
    "        j=j+1\n",
    "\n",
    "p1, = plt.plot(np.mean(yhat_test[:,:,0].T,axis=1),'r')\n",
    "p2, = plt.plot(np.mean(yhat_train[:,:,0].T,axis=1),'b')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(12,5)\n",
    "plt.xlabel('Complexity')\n",
    "plt.ylabel('Error rate')\n",
    "plt.legend([p1, p2], [\"Testing error\", \"Training error\"])\n",
    "plt.savefig(\"learning_curve_4.png\",dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos usar todos estos conceptos para comprender y seleccionar la complejidad de un modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-19T14:23:04.229180Z",
     "start_time": "2018-09-19T14:22:46.490573Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%reset -f\n",
    "%matplotlib inline\n",
    "import pickle\n",
    "ofname = open('dataset_small.pkl','rb') \n",
    "(x,y) = pickle.load(ofname, encoding='latin1')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn import tree\n",
    "from sklearn import cross_validation\n",
    "\n",
    "#\n",
    "\n",
    "#Create a 10-fold cross validation set\n",
    "kf=cross_validation.KFold(n=y.shape[0], n_folds=10, shuffle=True, random_state=0)\n",
    "      \n",
    "#Search the parameter among the following\n",
    "C=np.arange(2,20,)\n",
    "\n",
    "acc = np.zeros((10,18))\n",
    "i=0\n",
    "for train_index, val_index in kf:\n",
    "    X_train, X_val = x[train_index], x[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "    j=0\n",
    "    for c in C:\n",
    "        dt = tree.DecisionTreeClassifier(min_samples_leaf=1, max_depth=c)\n",
    "        dt.fit(X_train,y_train)\n",
    "        yhat = dt.predict(X_val)\n",
    "        acc[i][j] = metrics.accuracy_score(yhat, y_val)\n",
    "        j=j+1\n",
    "    i=i+1\n",
    "    \n",
    "plt.boxplot(acc);\n",
    "for i in range(18):\n",
    "    xderiv = (i+1)*np.ones(acc[:,i].shape)+(np.random.rand(10,)-0.5)*0.1\n",
    "    plt.plot(xderiv,acc[:,i],'ro',alpha=0.3)\n",
    "\n",
    "print('Mean accuracy: ' + str(np.mean(acc,axis = 0)))\n",
    "print('Selected model index: ' + str(np.argmax(np.mean(acc,axis = 0))))\n",
    "print('Complexity: ' + str(C[np.argmax(np.mean(acc,axis = 0))]))\n",
    "plt.ylim((0.7,1.))\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(12,5)\n",
    "plt.xlabel('Complexity')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.savefig(\"model_selection.png\",dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-19T14:23:27.390073Z",
     "start_time": "2018-09-19T14:23:22.457112Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%reset -f\n",
    "%matplotlib inline\n",
    "import pickle\n",
    "ofname = open('dataset_small.pkl','rb') \n",
    "(x,y) = pickle.load(ofname, encoding='latin1')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn import tree\n",
    "from sklearn import cross_validation\n",
    "\n",
    "#Train_test split\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(x, y, test_size=0.20, random_state=42)\n",
    "\n",
    "#Create a 10-fold cross validation set\n",
    "kf=cross_validation.KFold(n=y_train.shape[0], n_folds=10, shuffle=True, random_state=0)     \n",
    "#Search the parameter among the following\n",
    "C=np.arange(2,20,)\n",
    "acc = np.zeros((10,18))\n",
    "i=0\n",
    "for train_index, val_index in kf:\n",
    "    X_t, X_val = X_train[train_index], X_train[val_index]\n",
    "    y_t, y_val = y_train[train_index], y_train[val_index]\n",
    "    j=0\n",
    "    for c in C:\n",
    "        dt = tree.DecisionTreeClassifier(min_samples_leaf=1, max_depth=c)\n",
    "        dt.fit(X_t,y_t)\n",
    "        yhat = dt.predict(X_val)\n",
    "        acc[i][j] = metrics.accuracy_score(yhat, y_val)\n",
    "        j=j+1\n",
    "    i=i+1\n",
    "\n",
    "print('Mean accuracy: ' + str(np.mean(acc,axis = 0)))\n",
    "print('Selected model index: ' + str(np.argmax(np.mean(acc,axis = 0))))\n",
    "print('Complexity: ' + str(C[np.argmax(np.mean(acc,axis = 0))]))\n",
    "\n",
    "\n",
    "#Train the model with the complete training set with the selected complexity\n",
    "dt = tree.DecisionTreeClassifier(min_samples_leaf=1, max_depth=C[np.argmax(np.mean(acc,axis = 0))])\n",
    "dt.fit(X_train,y_train)\n",
    "#Test the model with the test set \n",
    "yhat = dt.predict(X_test)\n",
    "print('Test accuracy: ' + str(metrics.accuracy_score(yhat, y_test)))\n",
    "\n",
    "#Train the model for handling to the client\n",
    "dt = tree.DecisionTreeClassifier(min_samples_leaf=1, max_depth=C[np.argmax(np.mean(acc,axis = 0))])\n",
    "dt.fit(x,y)\n",
    "\n",
    "plt.boxplot(acc);\n",
    "for i in range(18):\n",
    "    xderiv = (i+1)*np.ones(acc[:,i].shape)+(np.random.rand(10,)-0.5)*0.1\n",
    "    plt.plot(xderiv,acc[:,i],'ro',alpha=0.3)\n",
    "\n",
    "\n",
    "plt.ylim((0.7,1.))\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(12,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ADVERTENCIA: la siguiente celda tarda mucho tiempo en ejecutarse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-09-19T20:11:57.777Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%reset -f\n",
    "\n",
    "import pickle\n",
    "ofname = open('dataset_small.pkl','rb') \n",
    "(x,y) = pickle.load(ofname, encoding='latin1')\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import grid_search\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import svm\n",
    "from sklearn import linear_model\n",
    "from sklearn import cross_validation\n",
    "from sklearn import metrics\n",
    "\n",
    "parameters = {'C':[1e4,1e5,1e6],'gamma':[1e-5,1e-4,1e-3]}\n",
    "\n",
    "N_folds = 5\n",
    "\n",
    "kf=cross_validation.KFold(n=y.shape[0], n_folds=N_folds,  shuffle=True, random_state=0)\n",
    "\n",
    "acc = np.zeros((N_folds,))\n",
    "i=0\n",
    "#We will build the predicted y from the partial predictions on the test of each of the folds\n",
    "yhat = y.copy()\n",
    "for train_index, test_index in kf:\n",
    "    X_train, X_test = x[train_index,:], x[test_index,:]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    clf = svm.SVC(kernel='rbf')\n",
    "    clf = grid_search.GridSearchCV(clf, parameters, cv = 3) #This line does a cross-validation on the \n",
    "    clf.fit(X_train,y_train.ravel())\n",
    "    X_test = scaler.transform(X_test)\n",
    "    yhat[test_index] = clf.predict(X_test)\n",
    "    \n",
    "print(metrics.accuracy_score(yhat, y))\n",
    "print(metrics.confusion_matrix(yhat, y))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "144px",
    "left": "998px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
