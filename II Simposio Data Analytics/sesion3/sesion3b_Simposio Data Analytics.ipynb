{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis de sentimiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"> <sub>Referencia: Igual L. & Seguí, S. (2017). *Introduction to Data Science*. Springer. Código fuente: https://github.com/DataScienceUB/introduction-datascience-python-book </sub> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero, importemos algunas librerías útiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-20T01:42:55.893269Z",
     "start_time": "2018-09-20T01:42:55.654743Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "#matplotlib inline \n",
    "#plt.style.use('seaborn-whitegrid')\n",
    "plt.rc('text', usetex=True)\n",
    "plt.rc('font', family='times')\n",
    "plt.rc('xtick', labelsize=10) \n",
    "plt.rc('ytick', labelsize=10) \n",
    "plt.rc('font', size=12) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requerimientos de instalación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-20T01:46:18.053473Z",
     "start_time": "2018-09-20T01:46:14.233036Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unidecode\n",
      "  Downloading https://files.pythonhosted.org/packages/59/ef/67085e30e8bbcdd76e2f0a4ad8151c13a2c5bce77c85f8cad6e1f16fb141/Unidecode-1.0.22-py2.py3-none-any.whl (235kB)\n",
      "\u001b[K    100% |████████████████████████████████| 235kB 1.1MB/s ta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: unidecode\n",
      "Successfully installed unidecode-1.0.22\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 18.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de comenzar, es importante descargar los nltk corpora. Puede descargar paquetes de datos individuales o puede descargar la colección completa (usando \"all\"). Los corpus útiles para este cuaderno incluyen wordnet, movie_reviews y stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-20T01:46:03.539467Z",
     "start_time": "2018-09-20T01:45:49.409856Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/hjrosasq/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "#nltk.download()\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejemplo 1:\n",
    "Ejemplo de Stemmer / Lemmatizer usando NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-20T01:51:05.932425Z",
     "start_time": "2018-09-20T01:51:05.845315Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Here', 'are', 'some', 'very', 'simple', 'basic', 'sentences'], ['They', 'wo', 'nt', 'be', 'very', 'interesting', 'I', 'm', 'afraid'], ['The', 'point', 'of', 'these', 'examples', 'is', 'to', 'learn', 'how', 'basic', 'text', 'cleaning', 'works', 'on', 'very', 'simple', 'data']]\n",
      "[['here', 'are', 'some', 'veri', 'simpl', 'basic', 'sentenc'], ['they', 'wo', 'nt', 'be', 'veri', 'interest', 'I', 'm', 'afraid'], ['the', 'point', 'of', 'these', 'exampl', 'is', 'to', 'learn', 'how', 'basic', 'text', 'clean', 'work', 'on', 'veri', 'simpl', 'data']]\n"
     ]
    }
   ],
   "source": [
    "raw_docs = [\"Here are some very simple basic sentences.\", \"They won't be very interesting, I'm afraid.\", \"The point of these examples is to _learn how basic text cleaning works_ on *very simple* data.\"]\n",
    "            \n",
    "from nltk.tokenize import word_tokenize\n",
    "tokenized_docs = [word_tokenize(doc) for doc in raw_docs]\n",
    "\n",
    "import re\n",
    "import string\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "tokenized_docs_no_punctuation = []\n",
    "for review in tokenized_docs:\n",
    "    new_review = []\n",
    "    for token in review:\n",
    "        new_token = regex.sub(u'', token)\n",
    "        if not new_token == u'':\n",
    "            new_review.append(new_token)\n",
    "    tokenized_docs_no_punctuation.append(new_review)\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "porter = PorterStemmer()\n",
    "snowball = SnowballStemmer('english')\n",
    "wordnet = WordNetLemmatizer()\n",
    "\n",
    "#snowball.stem(porter)\n",
    "#snowball.stem(snoball)\n",
    "#wordnet.lemmatize(wordnet)\n",
    "\n",
    "preprocessed_docs = []\n",
    "for doc in tokenized_docs_no_punctuation:\n",
    "    final_doc = []\n",
    "    for word in doc:\n",
    "        final_doc.append(porter.stem(word))\n",
    "        # final_doc.append(snowball.stem(word))\n",
    "        # requires 'corpora/wordnet' -> nltk.download()\n",
    "        # final_doc.append(wordnet.lemmatize(word))\n",
    "        # requires 'corpora/wordnet' -> nltk.download()\n",
    "    preprocessed_docs.append(final_doc)\n",
    "\n",
    "print(tokenized_docs_no_punctuation)\n",
    "print(preprocessed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estos ejemplos usan funciones de los módulos \"PorterStemmer\", \"SnowballStemmer\" y \"WordNetLemmatizer\". Los resultados de los tres enfoques son casi equivalentes. Tenga en cuenta que el script busca todos los elementos (palabras) en la lista \"tokenized \\ _docs \\ _no \\ _punctuation\", y para cada uno de ellos se puede considerar uno de los diferentes enfoques. Tenga en cuenta que los dos últimos enfoques de \"SnowballStemmer\" y \"WordNetLemmatizer\" requieren la instalación de corpus adicionales de NLTK, que se pueden descargar ejecutando \"nltk.download ()\". En este ejemplo, estamos ejecutando el primer enfoque \"porter.stem (word)\" añadiendo los nuevos resultados de stemmer o lemmatizer de \"word\" en la nueva lista \"final \\ _doc\" que finalmente se incluye en\n",
    "\"\\ docs preprocesados\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 2: Vector de características de frecuencias de palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-20T01:56:12.891882Z",
     "start_time": "2018-09-20T01:56:12.828213Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('Mireia', 1), ('loves', 2), ('me', 2), ('more', 1), ('than', 1), ('Hector', 1)])\n",
      "dict_items([('Sergio', 1), ('likes', 1), ('me', 2), ('more', 1), ('than', 1), ('Mireia', 1), ('loves', 1)])\n",
      "dict_items([('He', 1), ('likes', 1), ('basketball', 1), ('more', 1), ('than', 1), ('footbal', 1)])\n",
      "Our vocabulary vector is [He, Hector, basketball, than, more, likes, footbal, Sergio, me, Mireia, loves]\n",
      "The doc is \"Mireia loves me more than Hector loves me\"\n",
      "The tf vector for Document 1 is [0, 1, 0, 1, 1, 0, 0, 0, 2, 1, 2]\n",
      "The doc is \"Sergio likes me more than Mireia loves me\"\n",
      "The tf vector for Document 2 is [0, 0, 0, 1, 1, 1, 0, 1, 2, 1, 1]\n",
      "The doc is \"He likes basketball more than footbal\"\n",
      "The tf vector for Document 3 is [1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0]\n",
      "All combined, here is our master document term matrix: \n",
      "[[0, 1, 0, 1, 1, 0, 0, 0, 2, 1, 2], [0, 0, 0, 1, 1, 1, 0, 1, 2, 1, 1], [1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "mydoclist = ['Mireia loves me more than Hector loves me', 'Sergio likes me more than Mireia loves me', 'He likes basketball more than footbal']\n",
    "from collections import Counter\n",
    "for doc in mydoclist:\n",
    "    tf = Counter()\n",
    "    for word in doc.split():\n",
    "        tf[word] +=1\n",
    "    print(tf.items())\n",
    "    \n",
    "def build_lexicon(corpus):  # define a set with all possible words included\n",
    "                            # in all the sentences or \"corpus\"\n",
    "    lexicon = set()\n",
    "    for doc in corpus:\n",
    "        lexicon.update([word for word in doc.split()])\n",
    "    return lexicon\n",
    "def tf(term, document):\n",
    "    return freq(term, document)\n",
    "def freq(term, document):\n",
    "    return document.split().count(term)\n",
    "vocabulary = build_lexicon(mydoclist)\n",
    "doc_term_matrix = []\n",
    "print('Our vocabulary vector is [' + ', '.join(list(vocabulary)) + ']')\n",
    "for doc in mydoclist:\n",
    "    print('The doc is \"' + doc + '\"')\n",
    "    tf_vector = [tf(word, doc) for word in vocabulary]\n",
    "    tf_vector_string = ', '.join(format(freq, 'd') for freq in tf_vector)\n",
    "    print('The tf vector for Document %d is [%s]' % ((mydoclist.index(doc)+1), tf_vector_string))\n",
    "    doc_term_matrix.append(tf_vector)\n",
    "    \n",
    "print('All combined, here is our master document term matrix: ')\n",
    "print(doc_term_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Según el script anterior, cada documento está en el mismo espacio de características, lo que significa que\n",
    "puede representar todo el corpus en el mismo espacio dimensional\n",
    "sin haber perdido demasiada información. Una vez que tenemos los datos en\n",
    "el mismo espacio de características, podemos comenzar a aplicar alguna máquina\n",
    "métodos de aprendizaje: clasificación, agrupamiento, etc. Pero\n",
    "de hecho, tenemos algunos problemas. Las palabras no son todas igualmente\n",
    "informativo. Si las palabras aparecen con demasiada frecuencia en un solo documento,\n",
    "van a arruinar nuestro análisis. Queremos realizar algunos\n",
    "escala de cada uno de estos vectores de frecuencia de término en algo una\n",
    "un poco más representativo. En otras palabras, tenemos que hacer algo\n",
    "vectorización. Una posibilidad es asegurar que la norma L2\n",
    "de cada vector es igual a 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 3: Normalización L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-20T01:57:11.918393Z",
     "start_time": "2018-09-20T01:57:11.890881Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A regular old document term matrix: \n",
      "[[0 1 0 1 1 0 0 0 2 1 2]\n",
      " [0 0 0 1 1 1 0 1 2 1 1]\n",
      " [1 0 1 1 1 1 1 0 0 0 0]]\n",
      "\n",
      "A document term matrix with row-wise L2 norms of 1:\n",
      "[[0.         0.28867513 0.         0.28867513 0.28867513 0.\n",
      "  0.         0.         0.57735027 0.28867513 0.57735027]\n",
      " [0.         0.         0.         0.31622777 0.31622777 0.31622777\n",
      "  0.         0.31622777 0.63245553 0.31622777 0.31622777]\n",
      " [0.40824829 0.         0.40824829 0.40824829 0.40824829 0.40824829\n",
      "  0.40824829 0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "def l2_normalizer(vec):\n",
    "    denom = np.sum([el**2 for el in vec])\n",
    "    return [(el / math.sqrt(denom)) for el in vec]\n",
    "doc_term_matrix_l2 = []\n",
    "for vec in doc_term_matrix:\n",
    "    doc_term_matrix_l2.append(l2_normalizer(vec))\n",
    "print('A regular old document term matrix: ')\n",
    "print(np.matrix(doc_term_matrix))\n",
    "print('\\nA document term matrix with row-wise L2 norms of 1:')\n",
    "print(np.matrix(doc_term_matrix_l2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puedes ver que hemos reducido los vectores para que cada elemento esté entre [0, 1], sin perder demasiado\n",
    "Información valiosa.\n",
    "\n",
    "Finalmente, tenemos una tarea restante por realizar. Así como no todas las palabras\n",
    "son igualmente valiosos dentro de un documento, no todas las palabras son\n",
    "valioso en todos los documentos. Podemos intentar reponderar cada palabra\n",
    "por su frecuencia de documento inversa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 4: Peso de características."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-20T01:58:13.690464Z",
     "start_time": "2018-09-20T01:58:13.674905Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our vocabulary vector is [He, Hector, basketball, than, more, likes, footbal, Sergio, me, Mireia, loves]\n",
      "The inverse document frequency vector is [1.098612, 1.098612, 1.098612, 0.000000, 0.000000, 0.405465, 1.098612, 1.098612, 0.405465, 0.405465, 0.405465]\n"
     ]
    }
   ],
   "source": [
    "def numDocsContaining(word, doclist):\n",
    "    doccount = 0\n",
    "    for doc in doclist:\n",
    "        if freq(word, doc) > 0:\n",
    "            doccount +=1\n",
    "    return doccount\n",
    "def idf(word, doclist):\n",
    "    n_samples = len(doclist)\n",
    "    df = numDocsContaining(word, doclist)\n",
    "    return np.log(n_samples / (float(df)) )\n",
    "my_idf_vector = [idf(word, mydoclist) for word in vocabulary]\n",
    "print('Our vocabulary vector is [' + ', '.join(list(vocabulary)) + ']')\n",
    "print('The inverse document frequency vector is [' + ', '.join(format(freq, 'f') for freq in my_idf_vector) + ']')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejemplo 5: Código de reconocimiento de análisis de sentimiento binario de críticos de películas\n",
    "\n",
    "En este ejemplo, aplicamos todo el proceso de análisis de sentimiento a la película grande.\n",
    "conjunto de datos (http://www.aclweb.org/anthology/P11-1015). éste es uno de\n",
    "los conjuntos de datos disponibles públicos más grandes para el análisis de sentimiento, que\n",
    "incluye más de 50,000 textos de reseñas de películas, incluida la\n",
    "anotación de verdad de suelo relacionada con película positiva y negativa\n",
    "revisión. Como prueba de concepto para este ejemplo, usamos un subconjunto de\n",
    "el conjunto de datos que consiste en aproximadamente el 10% de los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puede usar los siguientes comandos en un sistema operativo Linux para descargar los datos requeridos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-20T02:00:58.401887Z",
     "start_time": "2018-09-20T01:59:52.018052Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-09-19 20:59:52--  http://ai.stanford.edu/~amaas//data/sentiment/aclImdb_v1.tar.gz\n",
      "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
      "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 84125825 (80M) [application/x-gzip]\n",
      "Saving to: ‘aclImdb_v1.tar.gz’\n",
      "\n",
      "aclImdb_v1.tar.gz   100%[===================>]  80,23M  1,24MB/s    in 66s     \n",
      "\n",
      "2018-09-19 21:00:58 (1,22 MB/s) - ‘aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://ai.stanford.edu/~amaas//data/sentiment/aclImdb_v1.tar.gz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-20T02:01:19.559125Z",
     "start_time": "2018-09-20T02:01:10.346860Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘train’: File exists\n",
      "mkdir: cannot create directory ‘train/pos2’: File exists\n",
      "mkdir: cannot create directory ‘train/neg2’: File exists\n",
      "mkdir: cannot create directory ‘test’: File exists\n",
      "mkdir: cannot create directory ‘test/pos2’: File exists\n",
      "mkdir: cannot create directory ‘test/neg2’: File exists\n"
     ]
    }
   ],
   "source": [
    "!tar -xf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para sistemas de Windows, puede descargar los datos aquí: http://ai.stanford.edu/~amaas/data/sentiment/\n",
    "Dentro del archivo comprimido encontrarás varios críticos de cine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las siguientes líneas de código seleccionarán un subconjunto de las críticas del conjunto de datos para ejecutar el siguiente ejemplo de reconocimiento de análisis de sentimiento binario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-20T02:03:59.829871Z",
     "start_time": "2018-09-20T02:03:59.708998Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "files=5\n",
    "count=0\n",
    "for file in os.listdir(\"aclImdb/train/pos/\"):\n",
    "    if count > files:\n",
    "        break\n",
    "    if file.endswith(\".txt\"):\n",
    "        os.rename('aclImdb/train/pos/' + file, 'train/pos2/' + file)\n",
    "    count=count+1\n",
    "count=0\n",
    "for file in os.listdir(\"aclImdb/train/neg/\"):\n",
    "    if count > files:\n",
    "        break\n",
    "    if file.endswith(\".txt\"):\n",
    "        os.rename('aclImdb/train/neg/' + file, 'train/neg2/' + file)\n",
    "    count=count+1\n",
    "count=0\n",
    "for file in os.listdir(\"aclImdb/test/pos/\"):\n",
    "    if count > files:\n",
    "        break\n",
    "    if file.endswith(\".txt\"):\n",
    "        os.rename('aclImdb/test/pos/' + file, 'test/pos2/' + file)\n",
    "    count=count+1\n",
    "count=0\n",
    "for file in os.listdir(\"aclImdb/test/neg/\"):\n",
    "    if count > files:\n",
    "        break\n",
    "    if file.endswith(\".txt\"):\n",
    "        os.rename('aclImdb/test/neg/' + file, 'test/neg2/' + file)\n",
    "    count=count+1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y el próximo script realizará todo el entrenamiento y el procedimiento de prueba en el subconjunto seleccionado del conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-20T02:07:51.390886Z",
     "start_time": "2018-09-20T02:07:50.449724Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the training data positive\n",
      "Reading the training data negative\n",
      "Defining dictionaries\n",
      "Reading the test data positive\n",
      "Reading the test data negative\n",
      "Computing test feature vectors\n",
      "Training and testing on training Naive Bayes\n",
      "Number of mislabeled training points out of a total 12 points : 0\n",
      "Training and testing on test Naive Bayes\n",
      "Number of mislabeled test points out of a total 12 points : 2\n",
      "Training and testing on train with SVM\n",
      "Number of mislabeled test points out of a total 12 points : 0\n",
      "Testing on test with already trained SVM\n",
      "Number of mislabeled test points out of a total 12 points : 7\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import svm\n",
    "from unidecode import unidecode\n",
    "import time\n",
    "\n",
    "def BoW():\n",
    "    # Tokenizing text\n",
    "    text_tokenized = [word_tokenize(doc) for doc in text]\n",
    "    # Removing punctuation\n",
    "    regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    tokenized_docs_no_punctuation = []\n",
    "    for review in text_tokenized:\n",
    "        new_review = []\n",
    "        for token in review:\n",
    "            new_token = regex.sub(u'', token)\n",
    "            if not new_token == u'':\n",
    "                new_review.append(new_token)\n",
    "        tokenized_docs_no_punctuation.append(new_review)\n",
    "    # Stemming and Lemmatizing\n",
    "    porter = PorterStemmer()\n",
    "    preprocessed_docs = []\n",
    "    for doc in tokenized_docs_no_punctuation:\n",
    "        final_doc = ''\n",
    "        for word in doc:\n",
    "            final_doc = final_doc + ' ' + porter.stem(word)\n",
    "        preprocessed_docs.append(final_doc)\n",
    "    return preprocessed_docs\n",
    "\n",
    "print('Reading the training data positive')\n",
    "text = []\n",
    "for file in os.listdir(\"train/pos2/\"):\n",
    "    if file.endswith(\".txt\"):\n",
    "        infile = open('train/pos2/' + file, 'r')\n",
    "        text.append(unidecode(infile.read()))\n",
    "        infile.close()\n",
    "num_posTrain=len(text)\n",
    "\n",
    "print('Reading the training data negative')\n",
    "\n",
    "for file in os.listdir(\"train/neg2/\"):\n",
    "    if file.endswith(\".txt\"):\n",
    "        infile = open('train/neg2/' + file, 'r')\n",
    "        text.append(unidecode(infile.read()))\n",
    "        infile.close()\n",
    "num_Train=len(text)\n",
    "\n",
    "print('Defining dictionaries')\n",
    "\n",
    "preprocessed_docs=BoW()\n",
    "# Computing TIDF word space\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df = 1)\n",
    "trainData = tfidf_vectorizer.fit_transform(preprocessed_docs)\n",
    "\n",
    "# Reading the test data\n",
    "\n",
    "print('Reading the test data positive')\n",
    "\n",
    "text = []\n",
    "for file in os.listdir(\"test/pos2/\"):\n",
    "    if file.endswith(\".txt\"):\n",
    "        infile = open('test/pos2/' + file, 'r')\n",
    "        text.append(unidecode(infile.read()))\n",
    "        infile.close()\n",
    "num_posTest=len(text)\n",
    "\n",
    "print('Reading the test data negative')\n",
    "\n",
    "for file in os.listdir(\"test/neg2/\"):\n",
    "    if file.endswith(\".txt\"):\n",
    "        infile = open('test/neg2/' + file, 'r')\n",
    "        text.append(unidecode(infile.read()))\n",
    "        infile.close()\n",
    "num_Test=len(text)\n",
    "\n",
    "print('Computing test feature vectors')\n",
    "start_time = time.time()\n",
    "\n",
    "preprocessed_docs=BoW()\n",
    "testData = tfidf_vectorizer.transform(preprocessed_docs)\n",
    "\n",
    "targetTrain = []\n",
    "for i in range(0,num_posTrain):\n",
    "    targetTrain.append(0)\n",
    "for i in range(0,num_Train-num_posTrain):\n",
    "    targetTrain.append(1)\n",
    "\n",
    "targetTest = []\n",
    "for i in range(0,num_posTest):\n",
    "    targetTest.append(0)\n",
    "for i in range(0,num_Test-num_posTest):\n",
    "    targetTest.append(1)\n",
    "\n",
    "print('Training and testing on training Naive Bayes')\n",
    "start_time = time.time()\n",
    "\n",
    "gnb = GaussianNB()\n",
    "testData.todense()\n",
    "y_pred = gnb.fit(trainData.todense(), targetTrain).predict(trainData.todense())\n",
    "print(\"Number of mislabeled training points out of a total %d points : %d\" % (trainData.shape[0],(targetTrain != y_pred).sum()))\n",
    "\n",
    "print('Training and testing on test Naive Bayes')\n",
    "\n",
    "y_pred = gnb.fit(trainData.todense(), targetTrain).predict(testData.todense())\n",
    "print(\"Number of mislabeled test points out of a total %d points : %d\" % (testData.shape[0],(targetTest != y_pred).sum()))\n",
    "\n",
    "print('Training and testing on train with SVM')\n",
    "clf = svm.SVC()\n",
    "clf.fit(trainData.todense(), targetTrain)\n",
    "y_pred = clf.predict(trainData.todense())\n",
    "print(\"Number of mislabeled test points out of a total %d points : %d\" % (trainData.shape[0],(targetTrain != y_pred).sum()))\n",
    "\n",
    "print('Testing on test with already trained SVM')\n",
    "y_pred = clf.predict(testData.todense())\n",
    "print(\"Number of mislabeled test points out of a total %d points : %d\" % (testData.shape[0],(targetTest != y_pred).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El ejemplo anterior usa uno de los conjuntos de datos disponibles públicos más grandes para el análisis de sentimiento, que\n",
    "incluye más de 50,000 textos de reseñas de películas, incluida la\n",
    "anotación de verdad de suelo relacionada con película positiva y negativa\n",
    "revisiones. Como prueba de concepto, para este ejemplo usamos un subconjunto de\n",
    "el conjunto de datos que consiste en aproximadamente el 30% de los datos. El código anterior reutiliza parte de los ejemplos anteriores para la limpieza de datos,\n",
    "lee los datos de entrenamiento y prueba de las carpetas proporcionadas por\n",
    "autores del conjunto de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejemplo 6: código de reconocimiento de análisis de sentimiento binario de Tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, veamos otro ejemplo simple de análisis de sentimiento\n",
    "basado en tweets. Aunque hay algo de trabajo que usa más\n",
    "tuitear datos\n",
    "http://www.sananalytics.com/lab/twitter-sentiment/ aquí\n",
    "se presenta un conjunto simple de tweets que se analizan como en el\n",
    "ejemplo anterior de críticas de películas. El código principal sigue siendo el mismo\n",
    "a excepción de la definición de los datos iniciales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-20T02:10:07.670952Z",
     "start_time": "2018-09-20T02:10:07.433399Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled training points out of a total 10 points : 0\n",
      "Training and testing on test Naive Bayes\n",
      "Number of mislabeled test points out of a total 6 points : 2\n",
      "Training and testing on train with SVM\n",
      "Number of mislabeled test points out of a total 10 points : 0\n",
      "Testing on test with already trained SVM\n",
      "Number of mislabeled test points out of a total 6 points : 2\n"
     ]
    }
   ],
   "source": [
    "def BoW():\n",
    "    # Tokenizing text\n",
    "    text_tokenized = [word_tokenize(doc) for doc in text]\n",
    "    # Removing punctuation\n",
    "    regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    tokenized_docs_no_punctuation = []\n",
    "    for review in text_tokenized:\n",
    "        new_review = []\n",
    "        for token in review:\n",
    "            new_token = regex.sub(u'', token)\n",
    "            if not new_token == u'':\n",
    "                new_review.append(new_token)\n",
    "        tokenized_docs_no_punctuation.append(new_review)\n",
    "    # Stemming and Lemmatizing\n",
    "    porter = PorterStemmer()\n",
    "    preprocessed_docs = []\n",
    "    for doc in tokenized_docs_no_punctuation:\n",
    "        final_doc = ''\n",
    "        for word in doc:\n",
    "            final_doc = final_doc + ' ' + porter.stem(word)\n",
    "        preprocessed_docs.append(final_doc)\n",
    "    return preprocessed_docs\n",
    "\n",
    "text = ['I love this sandwich.', 'This is an amazing place!',\n",
    "        'I feel very good about these beers.',\n",
    "         'This is my best work.', 'What an awesome view', 'I do not like this restaurant',\n",
    "         'I am tired of this stuff.', 'I can not deal with this', 'He is my sworn enemy!',\n",
    "         'My boss is horrible.']\n",
    "\n",
    "targetTrain = [0,0,0,0,0,1,1,1,1,1]\n",
    "preprocessed_docs=BoW()\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df = 1)\n",
    "trainData = tfidf_vectorizer.fit_transform(preprocessed_docs)\n",
    "\n",
    "text = ['The beer was good.', 'I do not enjoy my job', 'I aint feeling dandy today',\n",
    "        'I feel amazing!'\n",
    "        ,'Gary is a friend of mine.', 'I can not believe I am doing this.']\n",
    "targetTest = [0,1,1,0,0,1]\n",
    "preprocessed_docs=BoW()\n",
    "testData = tfidf_vectorizer.transform(preprocessed_docs)\n",
    "\n",
    "gnb = GaussianNB()\n",
    "testData.todense()\n",
    "y_pred = gnb.fit(trainData.todense(), targetTrain).predict(trainData.todense())\n",
    "print(\"Number of mislabeled training points out of a total %d points : %d\" % (trainData.shape[0],(targetTrain != y_pred).sum()))\n",
    "\n",
    "print('Training and testing on test Naive Bayes')\n",
    "\n",
    "y_pred = gnb.fit(trainData.todense(), targetTrain).predict(testData.todense())\n",
    "print(\"Number of mislabeled test points out of a total %d points : %d\" % (testData.shape[0],(targetTest != y_pred).sum()))\n",
    "\n",
    "print('Training and testing on train with SVM')\n",
    "clf = svm.SVC()\n",
    "clf.fit(trainData.todense(), targetTrain)\n",
    "y_pred = clf.predict(trainData.todense())\n",
    "print(\"Number of mislabeled test points out of a total %d points : %d\" % (trainData.shape[0],(targetTrain != y_pred).sum()))\n",
    "\n",
    "print('Testing on test with already trained SVM')\n",
    "y_pred = clf.predict(testData.todense())\n",
    "print(\"Number of mislabeled test points out of a total %d points : %d\" % (testData.shape[0],(targetTest != y_pred).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este escenario simple anterior, ambas estrategias de aprendizaje logran las mismas tasas de reconocimiento tanto en el entrenamiento como en los conjuntos de prueba.\n",
    "Tenga en cuenta que las palabras similares se comparten entre tweets. En la práctica, con ejemplos reales, los tweets incluirán oraciones no estructuradas y\n",
    "abreviaturas, haciendo el reconocimiento más difícil. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
